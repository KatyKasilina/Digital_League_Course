{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### находим индексы фейковых строк в тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "df_test.drop(['ID_code'], axis=1, inplace=True)\n",
    "df_test = df_test.values\n",
    "\n",
    "unique_samples = []\n",
    "unique_count = np.zeros_like(df_test)\n",
    "for feature in range(df_test.shape[1]):\n",
    "    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index_[count_ == 1], feature] += 1\n",
    "\n",
    "# Samples which have unique values are real the others are fake\n",
    "real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "\n",
    "print(len(real_samples_indexes))\n",
    "print(len(synthetic_samples_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### добавляем новые фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_col_list = df_train.columns[:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, target = df_train.iloc[:, 2:], df_train['target']\n",
    "\n",
    "test_id_code = df_test['ID_code']\n",
    "df_test.drop(['ID_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = df_train.values, df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 200), (200000, 200))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real = X_test[real_samples_indexes]\n",
    "X_all = np.vstack([X_train, X_test_real])\n",
    "X_all_df = pd.DataFrame(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 721 ms, total: 14.5 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "F_all = np.zeros_like(X_all, dtype=int)\n",
    "F_counts = np.zeros_like(X_all, dtype=int)\n",
    "\n",
    "for i, c in enumerate(X_all_df.columns):\n",
    "    a = X_all_df.groupby(c)[c].transform('count').values\n",
    "    F_all[:,i] = (a == 1).astype(int)\n",
    "    F_counts[:,i] = a / X_all_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(X, F, F_c):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.hstack([X, F[:len(X)], X*F[:len(X)] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_v2(X, F):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.hstack([X, F, X*F[:len(X)]]) ##, np.mean(X, axis=1)[:, None], np.std(X, axis=1)[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_features_v2(X_train, F_all[:orig_len])\n",
    "X_test_real = add_features_v2(X_test[real_samples_indexes], F_all[orig_len:])\n",
    "\n",
    "X_test_all = np.zeros((len(X_test), X_test_real.shape[1]))\n",
    "X_test_all[real_samples_indexes] = X_test_real\n",
    "X_test = X_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 600), (200000, 600))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_diff = ['var_0', 'var_2', 'var_6', 'var_9', 'var_12', 'var_13', 'var_22', 'var_26', 'var_40',\n",
    "                 'var_44', 'var_53','var_76', 'var_80', 'var_81', 'var_86', 'var_94', 'var_95', 'var_108', \n",
    "                 'var_109', 'var_110', 'var_112', 'var_115', 'var_123', 'var_131', 'var_133', 'var_139', \n",
    "                 'var_146', 'var_148', 'var_164', 'var_165', 'var_166','var_174', 'var_177', 'var_179', \n",
    "                 'var_131', 'var_180', 'var_184', 'var_188', 'var_131', 'var_190', 'var_191', 'var_195', 'var_198']\n",
    "\n",
    "##features_diff = orig_col_list\n",
    "\n",
    "df_train['dif_mean'] = df_train[features_diff].mean(axis = 1)\n",
    "df_train['dif_min'] = df_train[features_diff].min(axis = 1)\n",
    "df_train['dif_max'] = df_train[features_diff].max(axis = 1)\n",
    "\n",
    "df_train['dif_std'] = df_train[features_diff].std(axis = 1)\n",
    "df_train['dif_median'] = df_train[features_diff].median(axis = 1)\n",
    "df_train['dif_sum'] = df_train[features_diff].sum(axis = 1)\n",
    "\n",
    "df_test['dif_mean'] = df_test[features_diff].mean(axis = 1)\n",
    "df_test['dif_min'] = df_test[features_diff].min(axis = 1)\n",
    "df_test['dif_max'] = df_test[features_diff].max(axis = 1)\n",
    "\n",
    "df_test['dif_std'] = df_test[features_diff].std(axis = 1)\n",
    "df_test['dif_median'] = df_test[features_diff].median(axis = 1)\n",
    "df_test['dif_sum'] = df_test[features_diff].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_list = ['dif_mean', 'dif_min', 'dif_max', 'dif_std', 'dif_median', 'dif_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, df_train[new_feature_list].values])\n",
    "X_test = np.hstack([X_test, df_test[new_feature_list].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 606), (200000, 606))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.4, ##0.2,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05, ##0.1,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,  \n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13, ##15,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': 1,\n",
    "    'device': 'cpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'max_bin': 255,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train)\n",
    "test_df = pd.DataFrame(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\ttraining's auc: 0.902521\tvalid_1's auc: 0.884347\n",
      "[2000]\ttraining's auc: 0.915696\tvalid_1's auc: 0.892994\n",
      "[3000]\ttraining's auc: 0.924359\tvalid_1's auc: 0.897504\n",
      "[4000]\ttraining's auc: 0.931471\tvalid_1's auc: 0.900698\n",
      "[5000]\ttraining's auc: 0.937939\tvalid_1's auc: 0.903319\n",
      "[6000]\ttraining's auc: 0.943631\tvalid_1's auc: 0.905306\n",
      "[7000]\ttraining's auc: 0.948798\tvalid_1's auc: 0.906803\n",
      "[8000]\ttraining's auc: 0.953414\tvalid_1's auc: 0.908251\n",
      "[9000]\ttraining's auc: 0.957656\tvalid_1's auc: 0.909391\n",
      "[10000]\ttraining's auc: 0.961461\tvalid_1's auc: 0.910183\n",
      "[11000]\ttraining's auc: 0.964983\tvalid_1's auc: 0.910972\n",
      "[12000]\ttraining's auc: 0.968203\tvalid_1's auc: 0.911625\n",
      "[13000]\ttraining's auc: 0.971107\tvalid_1's auc: 0.912199\n",
      "[14000]\ttraining's auc: 0.973819\tvalid_1's auc: 0.912667\n",
      "[15000]\ttraining's auc: 0.976253\tvalid_1's auc: 0.913073\n",
      "[16000]\ttraining's auc: 0.978507\tvalid_1's auc: 0.913354\n",
      "[17000]\ttraining's auc: 0.980687\tvalid_1's auc: 0.913529\n",
      "[18000]\ttraining's auc: 0.982583\tvalid_1's auc: 0.913785\n",
      "[19000]\ttraining's auc: 0.984352\tvalid_1's auc: 0.913863\n",
      "[20000]\ttraining's auc: 0.985961\tvalid_1's auc: 0.914103\n",
      "[21000]\ttraining's auc: 0.987448\tvalid_1's auc: 0.914277\n",
      "[22000]\ttraining's auc: 0.988782\tvalid_1's auc: 0.914383\n",
      "[23000]\ttraining's auc: 0.990037\tvalid_1's auc: 0.914408\n",
      "[24000]\ttraining's auc: 0.991166\tvalid_1's auc: 0.914505\n",
      "[25000]\ttraining's auc: 0.992165\tvalid_1's auc: 0.914606\n",
      "[26000]\ttraining's auc: 0.993062\tvalid_1's auc: 0.9147\n",
      "[27000]\ttraining's auc: 0.993871\tvalid_1's auc: 0.914817\n",
      "[28000]\ttraining's auc: 0.994633\tvalid_1's auc: 0.914831\n",
      "[29000]\ttraining's auc: 0.995301\tvalid_1's auc: 0.914856\n",
      "[30000]\ttraining's auc: 0.995899\tvalid_1's auc: 0.914857\n",
      "[31000]\ttraining's auc: 0.996449\tvalid_1's auc: 0.914928\n",
      "[32000]\ttraining's auc: 0.996909\tvalid_1's auc: 0.914876\n",
      "[33000]\ttraining's auc: 0.997331\tvalid_1's auc: 0.914872\n",
      "Early stopping, best iteration is:\n",
      "[30484]\ttraining's auc: 0.996177\tvalid_1's auc: 0.91496\n"
     ]
    }
   ],
   "source": [
    "trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "num_round = 1000000\n",
    "clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training until validation scores don't improve for 3000 rounds\n",
    "[1000]\ttraining's auc: 0.900367\tvalid_1's auc: 0.882546\n",
    "[2000]\ttraining's auc: 0.918746\tvalid_1's auc: 0.894686\n",
    "[3000]\ttraining's auc: 0.928791\tvalid_1's auc: 0.899364\n",
    "[4000]\ttraining's auc: 0.935792\tvalid_1's auc: 0.901126\n",
    "[5000]\ttraining's auc: 0.941591\tvalid_1's auc: 0.901935\n",
    "[6000]\ttraining's auc: 0.946818\tvalid_1's auc: 0.902299\n",
    "[7000]\ttraining's auc: 0.951828\tvalid_1's auc: 0.90243\n",
    "[8000]\ttraining's auc: 0.956545\tvalid_1's auc: 0.902584\n",
    "[9000]\ttraining's auc: 0.960866\tvalid_1's auc: 0.90257\n",
    "[10000]\ttraining's auc: 0.964893\tvalid_1's auc: 0.902313\n",
    "Early stopping, best iteration is:\n",
    "[7656]\ttraining's auc: 0.954916\tvalid_1's auc: 0.902748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_df)\n",
    "\n",
    "sub_df = pd.DataFrame({\"ID_code\":  test_id_code.values})\n",
    "sub_df[\"target\"] = preds\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
